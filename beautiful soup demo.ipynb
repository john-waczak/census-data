{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial for how to use `beatiful soup` to scrape webpages. First, we use the `requests` library to grab the raw html from a url we specify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(\"https://www.google.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is wise to check that our request succeded. We can do so by checking the status_code attribute of the request object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(result.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out other info such as the http headers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Thu, 13 Aug 2020 16:35:35 GMT', 'Expires': '-1', 'Cache-Control': 'private, max-age=0', 'Content-Type': 'text/html; charset=ISO-8859-1', 'P3P': 'CP=\"This is not a P3P policy! See g.co/p3phelp for more info.\"', 'Content-Encoding': 'gzip', 'Server': 'gws', 'X-XSS-Protection': '0', 'X-Frame-Options': 'SAMEORIGIN', 'Set-Cookie': '1P_JAR=2020-08-13-16; expires=Sat, 12-Sep-2020 16:35:35 GMT; path=/; domain=.google.com; Secure, NID=204=rWuhyEKIlegFTimOAGKBt53dZL-lt2a0U4yGHgFCogkEJzPWU-a0uzXAMFoFutMSDYFxYBXbNntFsQ3BN1F7lGH-iNoaXPh2kOvQN6og2qFj0sTVmSvtFplcqzoURy0pmu6S6mv83gDWn8dBHJtr297J5PKc9KSkB0B-ywTtMCs; expires=Fri, 12-Feb-2021 16:35:35 GMT; path=/; domain=.google.com; HttpOnly', 'Alt-Svc': 'h3-29=\":443\"; ma=2592000,h3-27=\":443\"; ma=2592000,h3-T050=\":443\"; ma=2592000,h3-Q050=\":443\"; ma=2592000,h3-Q046=\":443\"; ma=2592000,h3-Q043=\":443\"; ma=2592000,quic=\":443\"; ma=2592000; v=\"46,43\"', 'Transfer-Encoding': 'chunked'}\n"
     ]
    }
   ],
   "source": [
    "print(result.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `domain` is correctly set to `google.com`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's store the `content` of the page in a variable so that we can make it easier to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Beautifule Soup to interact with content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(src, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images\n",
      "Maps\n",
      "Play\n",
      "YouTube\n",
      "News\n",
      "Gmail\n",
      "Drive\n",
      "More »\n",
      "Web History\n",
      "Settings\n",
      "Sign in\n",
      "Advanced search\n",
      "Take a Password Checkup\n",
      "Advertising Programs\n",
      "Business Solutions\n",
      "About Google\n",
      "Privacy\n",
      "Terms\n"
     ]
    }
   ],
   "source": [
    "# examine all the links\n",
    "links = soup.find_all('a')\n",
    "for link in links: \n",
    "    print(link.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/intl/en/about.html\">About Google</a> \n",
      "\n",
      "/intl/en/about.html\n"
     ]
    }
   ],
   "source": [
    "for link in links: \n",
    "    if \"About\" in link.text: \n",
    "        print(link, '\\n') \n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try a more elaborate example\n",
    "\n",
    "Task: obtain the links from the whitehouse briefings website. Extract all of the links on the page that point to the briefings and statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.whitehouse.gov/briefings-statements/\"\n",
    "result = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(result.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(src, 'lxml') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [] \n",
    "# grab all of the h2 tags\n",
    "for h2_tag in soup.find_all(\"h2\"):\n",
    "    a_tag = h2_tag.find('a') \n",
    "    urls.append(a_tag.attrs['href'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-secured-historic-deal-israel-united-arab-emirates-advance-peace-prosperity-region/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/joint-statement-united-states-state-israel-united-arab-emirates/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/remarks-president-trump-press-briefing-081320/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/statement-press-secretary-regarding-safe-reopening-americas-schools/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/remarks-president-trump-kids-first-getting-americas-children-safely-back-school/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/president-trump-announces-presidential-delegation-dominican-republic-attend-inauguration-excellency-luis-abinader/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-supporting-americas-students-families-encouraging-safe-reopening-americas-schools/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/remarks-president-trump-press-briefing-august-11-2020/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/president-donald-j-trump-using-every-available-resource-deliver-safe-effective-vaccine-american-people/ \n",
      "\n",
      "https://www.whitehouse.gov/briefings-statements/second-lady-karen-pence-highlights-suicide-prevention-visits-military-spouse-owned-business-charlotte-nc/ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for url in urls: \n",
    "    print(url, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
